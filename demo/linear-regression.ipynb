{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRv1XDqy-Mk"
      },
      "source": [
        "# Linear Regression using milliGrad\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mdehling/milligrad/blob/main/demo/linear-regression.ipynb)\n",
        "\n",
        "Make sure you have the `milliGrad` package installed.\n",
        "\n",
        "* If you are running this notebook on Google Colab, the first cell will take\n",
        "  care of it for you.\n",
        "* If you opened this notebook from within GitHub Codespaces, `milliGrad` should\n",
        "  already be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import milligrad\n",
        "    MISSING_MILLIGRAD = False\n",
        "except ImportError:\n",
        "    MISSING_MILLIGRAD = True\n",
        "\n",
        "try:\n",
        "    from google import colab\n",
        "    RUNNING_ON_COLAB = True\n",
        "except ImportError:\n",
        "    RUNNING_ON_COLAB = False\n",
        "\n",
        "if MISSING_MILLIGRAD and RUNNING_ON_COLAB:\n",
        "    !pip install -q 'git+https://github.com/mdehling/milligrad.git'\n",
        "elif MISSING_MILLIGRAD and not RUNNING_ON_COLAB:\n",
        "    raise ModuleNotFoundError(\"please install 'milligrad' package\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVD_f8x1Bf75"
      },
      "outputs": [],
      "source": [
        "from milligrad import Tensor, nn\n",
        "from milligrad.nn import functional as F\n",
        "from milligrad.datasets import make_linear\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 0: An Analytical Solution\n",
        "Linear regression has a simple analytical solution which minimizes the mean\n",
        "squared error: $[W\\,b]^T = (X^TX)^{-1}X^Ty$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p0, p1 = (1,3), (3,2)\n",
        "x, y = np.hsplit(make_linear(p0, p1, n=100, noise=0.1), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_linear_regression(x, y):\n",
        "    n, k = x.shape\n",
        "    x = np.hstack([x, np.ones((n,1))])\n",
        "    Wb = y.T @ x @ np.linalg.inv(x.T@x)\n",
        "    return Wb[:,:-1], Wb[:,-1]\n",
        "\n",
        "W, b = solve_linear_regression(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xs = np.array([[x.min()], [x.max()]])\n",
        "ys = xs@W + b\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xs, ys)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7dOHw7_9zTb"
      },
      "source": [
        "## Approach 1: Using just the Tensor Class\n",
        "Below we calculate an approximation to the optimal linear regression solution\n",
        "using gradient descent to repeatedly update the weights to minimize the mean\n",
        "squared error.  The approach taken here uses the basic `Tensor` class to\n",
        "calculate the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p0, p1 = (1,1), (3,5)\n",
        "x, y = np.hsplit(make_linear(p0, p1, n=100, noise=0.1), 2)\n",
        "x, y = Tensor(x, _label='x'), Tensor(y, _label='y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGO8YkKVMAVI"
      },
      "outputs": [],
      "source": [
        "# define weights\n",
        "W = Tensor(np.random.randn(1,1), requires_grad=True, _label='W')\n",
        "b = Tensor(np.zeros((1,1)), requires_grad=True, _label='b')\n",
        "\n",
        "lr = 1e-1\n",
        "for i in range(250):\n",
        "    # reset gradients to zero\n",
        "    W._zero_grad()\n",
        "    b._zero_grad()\n",
        "\n",
        "    # calculate loss and gradients\n",
        "    loss = F.mean_squared_error(x@W + b, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    W.value -= lr * W.grad\n",
        "    b.value -= lr * b.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91IUYNxMOcmY"
      },
      "outputs": [],
      "source": [
        "xs = np.array([[x.value.min()], [x.value.max()]])\n",
        "ys = xs@W.value + b.value\n",
        "\n",
        "plt.scatter(x.value, y.value)\n",
        "plt.plot(xs, ys)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ft7ua8-w6f"
      },
      "source": [
        "## Approach 2: Using a Neural Network\n",
        "In this section we reimplement the previous approach using a `Linear` neural\n",
        "network layer (perceptron) and the `SGD` optimizer.  The underlying math is\n",
        "exactly the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p0, p1 = (1,1), (3,-1)\n",
        "x, y = np.hsplit(make_linear(p0, p1, n=100, noise=0.1), 2)\n",
        "x, y = Tensor(x, _label='x'), Tensor(y, _label='y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcCNSQ88cVzF"
      },
      "outputs": [],
      "source": [
        "model = nn.Linear(1, 1)\n",
        "opt = nn.optim.SGD(model, lr=1e-1)\n",
        "\n",
        "for i in range(250):\n",
        "    opt.zero_grad()\n",
        "    loss = F.mean_squared_error(model(x), y)\n",
        "    loss.backward()\n",
        "    opt.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xs = np.array([[x.value.min()], [x.value.max()]])\n",
        "ys = model(xs).value\n",
        "\n",
        "plt.scatter(x.value, y.value)\n",
        "plt.plot(xs, ys)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
